<?xml version="1.0"
      encoding="UTF-8"
?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title> When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework and A New Benchmark </title>
<meta name="generator" content="Nested http://nestededitor.sourceforge.net/" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    menuSettings: {zoom: "Double-Click", zscale: "300%"},
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    MathMenu: {showRenderer: false},
    "HTML-CSS": {
        availableFonts: ["TeX"],
        preferredFont: "TeX",
        imageFont: null
    }
  });
</script>
<style type="text/css">
    body { background-color: White; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif; width:900px; margin:0 auto;}
    h1 { color: black; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif; }
    p { color: black; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif;}
    pre {
       overflow-x: auto;
       white-space: pre-wrap;
       white-space: -moz-pre-wrap;
       white-space: -pre-wrap;
       white-space: -o-pre-wrap;
       word-wrap: break-word;
    }

</style>
<style>
.aligncenter {
    text-align: center;
}
</style>


</head>
<body>

<p>&nbsp;</p>
<p>&nbsp;</p>
<div id="header" class="header" align="center">
<h1> When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework and A New Benchmark </h1>
<p style="text-align:center">
    <font size="4"> <a href="https://hzzone.github.io/">Zhizhong Huang</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <font size="4"> <a href="https://scholar.google.com/citations?user=Aib_NTYAAAAJ&hl=en">Junping Zhang</a>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <font size="4"> <a href="https://scholar.google.com/citations?user=RYfSzKwAAAAJ&hl=en">Hongming Shan</a>
    <p>
<p style="text-align:center">
<p style="text-align:center">
    <i><font size="4">
        Fudan University
    </font></i>
<p>
<p> <a href="https://cvpr2022.thecvf.com">CVPR 2022 <b>(Oral)</b> & TPAMI 2022 </a></p>
</div>


<hr class="heavy" />
<h2>Abstract</h2>
<div id="body" class="body">
<div id="section1" class="section">
<p>
To minimize the impact of age variation on face recognition, age-invariant face recognition (AIFR) extracts identity-related discriminative features by minimizing the correlation between identity- and age-related features while face age synthesis (FAS) eliminates age variation by converting the faces in different age groups to the same group. However, AIFR lacks visual results for model interpretation and FAS compromises downstream recognition due to artifacts. Therefore, we propose a unified, multi-task framework to jointly handle these two tasks, termed MTLFace, which can learn the age-invariant identity-related representation for face recognition while achieving pleasing face synthesis for model interpretation. Specifically, we propose an attention-based feature decomposition to decompose the mixed face features into two uncorrelated components—identity- and age-related features—in a spatially constrained way. Unlike the conventional one-hot encoding that achieves group-level FAS, we propose a novel identity conditional module to achieve identity-level FAS, which can improve the age smoothness of synthesized faces through a weight-sharing strategy. Benefiting from the proposed multi-task framework, we then leverage those high-quality synthesized faces from FAS to further boost AIFR via a novel selective fine-tuning strategy. Furthermore, to advance both AIFR and FAS, we collect and release a large cross-age face dataset with age and gender annotations, and a new benchmark specifically designed for tracing long-missing children. Extensive experimental results on five benchmark cross-age datasets demonstrate that MTLFace yields superior performance than state-of-the-art methods for both AIFR and FAS. We further validate MTLFace on two popular general face recognition datasets, obtaining competitive performance on face recognition in the wild.
</p>

<p>  
 <a href="https://arxiv.org/pdf/2210.09835.pdf">[PDF]</a>
 <a href="https://ieeexplore.ieee.org/document/9931965/">[DOI]</a> 
 <a href="https://github.com/Hzzone/MTLFace">[Code]</a> 
 <a href="https://mp.weixin.qq.com/s/ZmoHa_ImfyAAycjXIQYjSQ">[Chinese Blog]</a> 
</p>



<hr class="heavy" />
<h2>Demo</h2>

see notebook <a href="https://colab.research.google.com/github/Hzzone/MTLFace/blob/master/python_package/notebook/example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Explore in Colab"></a>

<img src='fig2.png' style='width: 300px'>

<hr class="heavy" />
<h2>Download</h2>

<ul>
<li>
<p>The pretrained models are available at Google Drive <a href="https://drive.google.com/file/d/1OmfAjP3BAqVxaQ2pwyJuOYUHy_incMNd/view?usp=share_link">https://drive.google.com/file/d/1OmfAjP3BAqVxaQ2pwyJuOYUHy_incMNd/view?usp=share_link</a> and BaiduDisk <a href="https://pan.baidu.com/s/1GT95OPNR-7GQPMSFFVqzeQ?pwd=wizs">https://pan.baidu.com/s/1GT95OPNR-7GQPMSFFVqzeQ?pwd=wizs</a></p>
</li>
<li>
<p>The SCAF dataset can be downloaded from Dropbox <a href="https://www.dropbox.com/s/o7zbnzlkpwczbg2/small.txt?dl=0">https://www.dropbox.com/s/o7zbnzlkpwczbg2/small.txt?dl=0</a></p>
</li>
<li>
<p>The ECAF dataset can be downloaded from Google Drive <a href="https://drive.google.com/file/d/1t5O3qbkXi-nD6lQjSHMTeMS2Elp0qUi1/view?usp=share_link">https://drive.google.com/file/d/1t5O3qbkXi-nD6lQjSHMTeMS2Elp0qUi1/view?usp=share_link</a> and BaiduDisk <a href="https://pan.baidu.com/s/1YHLRba_rLd70l3bJE-bIFg?pwd=7a2s">https://pan.baidu.com/s/1YHLRba_rLd70l3bJE-bIFg?pwd=7a2s</a></p>
</li>
</ul>
<p>The ECAF dataset is organized as follows:</p>
<ol>
<li><code>image_data/img_ffhq</code> and <code>image_data/img_mtcnn</code> are faces aligned by different key points such as FFHQ and MTCNN. Note that a few faces (~10) are not detected and are manually corrected by us.</li>
<li><code>image_data/data.json</code> contains the information of each image, including the source url, image url, and the attributes estimated by Face++ API.</li>
<li><code>image_pairs/*.txt</code> is the pairs used in the paper, following the LFW dataset.</li>
<li><code>source_code</code> contains the benchmarking, aligning code, etc.</li>
</ol>
<img src="fig1.png" style="width: 300px">


<hr class="heavy" />
<h2>Citation</h2>
<div id="body" class="body">
<div id="section8" class="section">
<p>If you found this code or our work useful please cite us:</p>
<pre style="font-size: 15px">
@inproceedings{huang2021age,
  title={When age-invariant face recognition meets face age synthesis: A multi-task learning framework},
  author={Huang, Zhizhong and Zhang, Junping and Shan, Hongming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7282--7291},
  year={2021}
}

@article{huang2022age,
  title={When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework and A New Benchmark},
  author={Huang, Zhizhong and Zhang, Junping and Shan, Hongming},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}
</pre>
</div>

